{
	"name": "nb_create_spb_blank_tables",
	"properties": {
		"folder": {
			"name": "nb_create_ecds_tables"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "esht",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f00c7638-b3f6-4b45-90f4-4c731274fdd6"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/528d7df3-243d-4d06-8d47-82c561237d91/resourceGroups/esht-project-rg/providers/Microsoft.Synapse/workspaces/louisworkspace/bigDataPools/esht",
				"name": "esht",
				"type": "Spark",
				"endpoint": "https://louisworkspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/esht",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# SPB Delta Tables Creation Notebook\r\n",
					"\r\n",
					"## Overview\r\n",
					"\r\n",
					"This notebook creates multiple **Delta tables** within the **SPB/ECDS** data lake using Microsoft Synapse Analytics. Each cell corresponds to a specific table setup with predefined schemas.\r\n",
					"\r\n",
					"## Tables Created\r\n",
					"\r\n",
					"1. **ClinicalNoteData_Current**\r\n",
					"2. **ClinicalNoteData_First**\r\n",
					"3. **PatientData_Current**\r\n",
					"4. **PatientVisitHistoryData_Current**\r\n",
					"5. **StoredProc_Log**\r\n",
					"6. **VisitArchiveID_delta**\r\n",
					"\r\n",
					"Each table is structured to meet specific data requirements, ensuring consistency and efficiency across the SPB data ecosystem.\r\n",
					"\r\n",
					"## Setting Default Values During Data Ingestion\r\n",
					"\r\n",
					"Default values are set using `df.withColumn`. It's recommended to implement these defaults **during data ingestion** to ensure consistency and reduce post-processing steps.\r\n",
					"\r\n",
					"**Guidance:**\r\n",
					"\r\n",
					"- **Integrate in ETL Pipelines:**\r\n",
					"  Incorporate `withColumn` transformations within your ETL processes.\r\n",
					"  ```python\r\n",
					"  df = df.withColumn(\"InsertedOn\", current_timestamp()) - all the tables\r\n",
					"\r\n",
					"## Primary keys and constraints in Delta tables\r\n",
					"\r\n",
					"Delta Lake does not support enforcing unique constraints or primary keys natively.\r\n",
					"To emulate primary key behavior, you need to handle duplicates during data ingestion \r\n",
					"\r\n",
					"**Guidance:**\r\n",
					"- **Implement data validation and data duplication checks in your data pipelines to ensure data integrity**\r\n",
					"```python\r\n",
					"df = df.filter(col(\"visitarchiveid\").isNotNull())\r\n",
					"df = df.dropDuplicates([\"visitarchiveid\"])\r\n",
					"```"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Import needed functions\r\n",
					"from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, ShortType, LongType\r\n",
					"from pyspark.sql.functions import current_timestamp"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Create table ClinicalNoteData_Current"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define the schema for the table\r\n",
					"schema = StructType([\r\n",
					"    StructField(\"visitarchiveid\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"NCDBEXID\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"Id\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"VisitId\", IntegerType(), nullable=True),\r\n",
					"    StructField(\"Timestamp\", TimestampType(), nullable=False),\r\n",
					"    StructField(\"Location\", StringType(), nullable=True),\r\n",
					"    StructField(\"PatientId\", IntegerType(), nullable=True),\r\n",
					"    StructField(\"HN\", StringType(), nullable=True),\r\n",
					"    StructField(\"NoteKey\", StringType(), nullable=False),\r\n",
					"    StructField(\"NoteValue\", StringType(), nullable=True),\r\n",
					"    StructField(\"NoteValShort\", StringType(), nullable=True),\r\n",
					"    StructField(\"InsertedOn\", TimestampType(), nullable=False),\r\n",
					"    StructField(\"EpisodeId\", IntegerType(), nullable=True),\r\n",
					"    StructField(\"NoteVal_LocalDateTime\", TimestampType(), nullable=True)\r\n",
					"])\r\n",
					"\r\n",
					"# Create an empty DataFrame with the defined schema\r\n",
					"df = spark.createDataFrame([], schema)\r\n",
					"\r\n",
					"# Define the path for the Delta table\r\n",
					"delta_table_path = \"abfss://curated@dlsrxcdevuksdp01.dfs.core.windows.net/ESHT/SPB/ClinicalNoteData_Current\"\r\n",
					"\r\n",
					"# Write the DataFrame as a Delta table\r\n",
					"df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_table_path)\r\n",
					"\r\n",
					"# Create Delta table\r\n",
					"spark.sql(f\"CREATE TABLE IF NOT EXISTS ClinicalNoteData_Current USING DELTA LOCATION '{delta_table_path}'\")\r\n",
					""
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Create table ClinicalNoteData_First"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define the schema for the table\r\n",
					"schema = StructType([\r\n",
					"    StructField(\"visitarchiveid\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"NCDBEXID\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"Id\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"VisitId\", IntegerType(), nullable=True),\r\n",
					"    StructField(\"Timestamp\", TimestampType(), nullable=False),\r\n",
					"    StructField(\"Location\", StringType(), nullable=True),\r\n",
					"    StructField(\"PatientId\", IntegerType(), nullable=True),\r\n",
					"    StructField(\"HN\", StringType(), nullable=True),\r\n",
					"    StructField(\"NoteKey\", StringType(), nullable=False),\r\n",
					"    StructField(\"NoteValue\", StringType(), nullable=True),\r\n",
					"    StructField(\"NoteValShort\", StringType(), nullable=True),\r\n",
					"    StructField(\"InsertedOn\", TimestampType(), nullable=False),\r\n",
					"    StructField(\"EpisodeId\", IntegerType(), nullable=True),\r\n",
					"    StructField(\"NoteVal_LocalDateTime\", TimestampType(), nullable=True)\r\n",
					"])\r\n",
					"\r\n",
					"# Create an empty DataFrame with the defined schema\r\n",
					"df = spark.createDataFrame([], schema)\r\n",
					"\r\n",
					"# Define the path for the Delta table (ensure correct casing)\r\n",
					"delta_table_path = \"abfss://curated@dlsrxcdevuksdp01.dfs.core.windows.net/ESHT/SPB/ClinicalNoteData_First\"\r\n",
					"\r\n",
					"# Write the DataFrame as a Delta table to initialize it\r\n",
					"df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_table_path)\r\n",
					"\r\n",
					"# Create Delta table in the metastore\r\n",
					"spark.sql(f\"CREATE TABLE IF NOT EXISTS ClinicalNoteData_First USING DELTA LOCATION '{delta_table_path}'\")"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Create table PatientData_Current"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define the schema for the table\r\n",
					"schema = StructType([\r\n",
					"    StructField(\"id\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"patientid\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"patientname\", StringType(), nullable=True),\r\n",
					"    StructField(\"dob\", TimestampType(), nullable=True),\r\n",
					"    StructField(\"hospnumber1\", StringType(), nullable=True),\r\n",
					"    StructField(\"hospnumber2\", StringType(), nullable=True),\r\n",
					"    StructField(\"altnumber\", StringType(), nullable=True),\r\n",
					"    StructField(\"natlnumber\", StringType(), nullable=True),\r\n",
					"    StructField(\"deceased\", ShortType(), nullable=True),\r\n",
					"    StructField(\"male\", ShortType(), nullable=True),\r\n",
					"    StructField(\"title\", StringType(), nullable=True),\r\n",
					"    StructField(\"maritalstatus\", StringType(), nullable=True),\r\n",
					"    StructField(\"religion\", StringType(), nullable=True),\r\n",
					"    StructField(\"ethnicity\", StringType(), nullable=True),\r\n",
					"    StructField(\"nationality\", StringType(), nullable=True),\r\n",
					"    StructField(\"addressStreet\", StringType(), nullable=True),\r\n",
					"    StructField(\"addressArea\", StringType(), nullable=True),\r\n",
					"    StructField(\"addressCity\", StringType(), nullable=True),\r\n",
					"    StructField(\"addressCounty\", StringType(), nullable=True),\r\n",
					"    StructField(\"addressPostCode\", StringType(), nullable=True),\r\n",
					"    StructField(\"addressCountry\", StringType(), nullable=True),\r\n",
					"    StructField(\"preferredTelephone\", StringType(), nullable=True),\r\n",
					"    StructField(\"dateTimeOfDeath\", TimestampType(), nullable=True),\r\n",
					"    StructField(\"lastName\", StringType(), nullable=True),\r\n",
					"    StructField(\"firstName\", StringType(), nullable=True),\r\n",
					"    StructField(\"middleName\", StringType(), nullable=True),\r\n",
					"    StructField(\"gender\", StringType(), nullable=True),\r\n",
					"    StructField(\"gpPracticeID\", StringType(), nullable=True),\r\n",
					"    StructField(\"unknownpatient\", ShortType(), nullable=True),\r\n",
					"    StructField(\"recordlastupdated\", TimestampType(), nullable=True),\r\n",
					"    StructField(\"InsertedOn\", TimestampType(), nullable=False)\r\n",
					"])\r\n",
					"\r\n",
					"# Create an empty DataFrame with the defined schema\r\n",
					"df = spark.createDataFrame([], schema)\r\n",
					"\r\n",
					"# Define the path for the Delta table (ensure correct casing)\r\n",
					"delta_table_path = \"abfss://curated@dlsrxcdevuksdp01.dfs.core.windows.net/ESHT/SPB/PatientData_Current\"\r\n",
					"\r\n",
					"# Write the DataFrame as a Delta table\r\n",
					"df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_table_path)\r\n",
					"\r\n",
					"# Create Delta table in the metastore\r\n",
					"spark.sql(f\"CREATE TABLE IF NOT EXISTS PatientData_Current USING DELTA LOCATION '{delta_table_path}'\")\r\n",
					""
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Create table PatientVisitHistoryData_Current"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define the schema for the table\r\n",
					"schema = StructType([\r\n",
					"    StructField(\"visitarchiveid\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"eventTime\", TimestampType(), nullable=False),\r\n",
					"    StructField(\"id\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"NCDBEXID\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"patientid\", IntegerType(), nullable=True),\r\n",
					"    StructField(\"HN\", StringType(), nullable=True),\r\n",
					"    StructField(\"ward\", StringType(), nullable=True),\r\n",
					"    StructField(\"area\", StringType(), nullable=True),\r\n",
					"    StructField(\"bed\", StringType(), nullable=True),\r\n",
					"    StructField(\"consultant\", StringType(), nullable=True),\r\n",
					"    StructField(\"speciality\", StringType(), nullable=True),\r\n",
					"    StructField(\"visitid\", StringType(), nullable=True),\r\n",
					"    StructField(\"mergedfrompatientid\", IntegerType(), nullable=True),\r\n",
					"    StructField(\"recordedTime\", TimestampType(), nullable=True),\r\n",
					"    StructField(\"eventType\", StringType(), nullable=True),\r\n",
					"    StructField(\"replacedBy\", IntegerType(), nullable=True),\r\n",
					"    StructField(\"timeto\", TimestampType(), nullable=True),\r\n",
					"    StructField(\"source\", StringType(), nullable=True),\r\n",
					"    StructField(\"RecordLastUpdated\", TimestampType(), nullable=True),\r\n",
					"    StructField(\"InsertedOn\", TimestampType(), nullable=False),\r\n",
					"    StructField(\"EpisodeId\", IntegerType(), nullable=True)\r\n",
					"])\r\n",
					"\r\n",
					"# Create an empty DataFrame with the defined schema\r\n",
					"df = spark.createDataFrame([], schema)\r\n",
					"\r\n",
					"# Define the path for the Delta table (ensure correct casing)\r\n",
					"delta_table_path = \"abfss://curated@dlsrxcdevuksdp01.dfs.core.windows.net/ESHT/SPB/PatientVisitHistoryData_Current\"\r\n",
					"\r\n",
					"# Write the DataFrame as a Delta table\r\n",
					"df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_table_path)\r\n",
					"\r\n",
					"# Create Delta table in the metastore\r\n",
					"spark.sql(f\"CREATE TABLE IF NOT EXISTS PatientVisitHistoryData_Current USING DELTA LOCATION '{delta_table_path}'\")\r\n",
					""
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Create table StoredProc_Log"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define the schema for the table\r\n",
					"schema = StructType([\r\n",
					"    StructField(\"ID\", LongType(), nullable=False),\r\n",
					"    StructField(\"Timestamp\", TimestampType(), nullable=False),\r\n",
					"    StructField(\"SP_Name\", StringType(), nullable=False),\r\n",
					"    StructField(\"MessageText\", StringType(), nullable=True)\r\n",
					"])\r\n",
					"\r\n",
					"# Create an empty DataFrame with the defined schema\r\n",
					"df = spark.createDataFrame([], schema)\r\n",
					"\r\n",
					"# Define the path for the Delta table (ensure correct casing)\r\n",
					"delta_table_path = \"abfss://curated@dlsrxcdevuksdp01.dfs.core.windows.net/ESHT/SPB/StoredProc_Log\"\r\n",
					"\r\n",
					"# Write the DataFrame as a Delta table to initialize it\r\n",
					"df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_table_path)\r\n",
					"\r\n",
					"# Create Delta table in the metastore\r\n",
					"spark.sql(f\"CREATE TABLE IF NOT EXISTS StoredProc_Log USING DELTA LOCATION '{delta_table_path}'\")"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Create table VisitArchiveID_delta"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\r\n",
					"\r\n",
					"# Define the schema for the table\r\n",
					"schema = StructType([\r\n",
					"    StructField(\"visitarchiveid\", IntegerType(), nullable=False),\r\n",
					"    StructField(\"InsertedOn\", TimestampType(), nullable=False)\r\n",
					"])\r\n",
					"\r\n",
					"# Create an empty DataFrame with the defined schema\r\n",
					"df = spark.createDataFrame([], schema)\r\n",
					"\r\n",
					"# Define the path for the Delta table (ensure correct casing)\r\n",
					"delta_table_path = \"abfss://curated@dlsrxcdevuksdp01.dfs.core.windows.net/ESHT/SPB/VisitArchiveID_delta\"\r\n",
					"\r\n",
					"# Write the DataFrame as a Delta table to initialize it\r\n",
					"df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_table_path)\r\n",
					"\r\n",
					"# Create Delta table in the metastore\r\n",
					"spark.sql(f\"CREATE TABLE IF NOT EXISTS VisitArchiveID_delta USING DELTA LOCATION '{delta_table_path}'\")"
				],
				"execution_count": 14
			}
		]
	}
}